{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from datetime import datetime,date, timedelta\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import Blob\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "client = storage.Client(project=\"ds5500\")\n",
    "\n",
    "INPUT_PATH = \"gs://kkbox-data/50_pct_undersample/\"\n",
    "bucket = client.get_bucket(\"kkbox-data\")\n",
    "split=\"test\" #use \"train\", \"val\" or \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs = pd.read_csv(INPUT_PATH + \"{}_user_logs.csv\".format(split))\n",
    "#user_logs = pd.read_csv(\"gs://dataprep-staging-a4a9adec-2491-40c1-8c86-fe48a78ef1f1/ribaudo.a@husky.neu.edu/jobrun/train_50_pct_user_logs.csv\")\n",
    "user_logs.loc[:,[\"date\"]]=pd.to_datetime(user_logs.date).dt.date\n",
    "members_df = pd.read_csv(INPUT_PATH + \"{}_members_transformed.csv\".format(split))\n",
    "y = members_df.loc[:,[\"msno\",\"is_churn\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove user_logs that don't exist in members df\n",
    "msno_diff = list(set(user_logs.msno) - set(members_df.msno)) #difference in 2 sets\n",
    "user_logs = user_logs[~user_logs[\"msno\"].isin(msno_diff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and clip the values at 2 * std because we want to be very sensitive at the lower end of the data.\n",
    "# In other words, it's safe to assume that those playing tracks above 2*std won't churn, we want to investigate the distribution of data below that point\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "for col in [\"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"]:\n",
    "    count_clip = np.clip(user_logs[col].values.reshape(-1,1), a_min=0,a_max=user_logs[col].values.reshape(-1,1).max())\n",
    "    #col_log = np.log(count_clip +.0001)\n",
    "    #col_std = std_scaler.fit_transform(col_log)\n",
    "    #col_clip = np.clip(col_std, a_min=col_std.min(),a_max=2)\n",
    "    #col_norm = max_scaler.fit_transform(col_clip)\n",
    "    #user_logs[col + \"_norm\"] = col_norm\n",
    "    user_logs[col + \"_clip\"] = count_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs = user_logs.drop([\"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = list(set(user_logs.msno))\n",
    "num_users = len(users)\n",
    "start_date = date(2016,7,1) # Technically we can go back further, but it introduces a lot of missing data for various users\n",
    "end_date = date(2017,1,31) # Max date we care about before evaluating churn\n",
    "#print(f\"Num dates: {num_dates}; num_users: {num_users}; padded records to create: {num_dates*num_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a list containing all dates from start to end\n",
    "dates = [start_date + timedelta(days=x) for x in range((end_date-start_date).days + 1)]\n",
    "num_dates=len(dates)\n",
    "# Join the user_log data with the dates created in the previous step\n",
    "padded_df = pd.DataFrame(product(users, dates), columns=[\"msno\",\"date\"])\n",
    "padded_df = padded_df.merge(user_logs, how='left', on=[\"msno\",\"date\"]).fillna(0)\n",
    "padded_df = padded_df.sort_values(by=[\"msno\",\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "padded_df_labeled = padded_df.merge(right=y,how=\"left\",on=\"msno\")\n",
    "\n",
    "padded_df_churn = padded_df_labeled[padded_df_labeled[\"is_churn\"] == 1]\n",
    "padded_df_no_churn = padded_df_labeled[padded_df_labeled[\"is_churn\"] == 0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metric = \"num_unq_clip\"\n",
    "\n",
    "mean_churn_by_date = padded_df_churn.loc[:,[\"date\",metric]].groupby(['date']).mean().reset_index()\n",
    "mean_no_churn_by_date = padded_df_no_churn.loc[:,[\"date\",metric]].groupby(['date']).mean().reset_index()\n",
    "\n",
    "mean_churn_by_date[metric + \"_centered\"] = mean_churn_by_date[metric] - mean_churn_by_date[metric].mean()\n",
    "mean_no_churn_by_date[metric + \"_centered\"] = mean_no_churn_by_date[metric] - mean_no_churn_by_date[metric].mean()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "sns.lineplot(data=mean_churn_by_date,x=\"date\", y=metric, label=\"Churn\")\n",
    "ax = sns.lineplot(data=mean_no_churn_by_date,x=\"date\", y=metric, label=\"No Churn\")\n",
    "ax.set(xlabel='Date', ylabel='Avg Unique Plays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.lineplot(data=mean_churn_by_date,x=\"date\", y=metric + \"_centered\", label=\"Churn\")\n",
    "ax = sns.lineplot(data=mean_no_churn_by_date,x=\"date\", y=metric + \"_centered\", label=\"No Churn\")\n",
    "ax.set(xlabel='Date', ylabel='Avg Unique Plays (Centered)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grouped_df = padded_df.drop(['num_25_norm', 'num_50_norm', 'num_75_norm',\n",
    "       'num_985_norm', 'num_100_norm', 'num_unq_norm', 'total_secs_norm'],axis=1)\n",
    "\n",
    "grouped_df = grouped_df.merge(right=y,how=\"left\",on=\"msno\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.lineplot(date=grouped_df,x=\"date\", y=\"sum_vals\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grouped_df = padded_df.drop(['num_25_norm', 'num_50_norm', 'num_75_norm',\n",
    "       'num_985_norm', 'num_100_norm', 'num_unq_norm', 'total_secs_norm'],axis=1)\n",
    "grouped_df[\"sum_vals\"] = padded_df.num_unq_norm #padded_df.drop([\"msno\",\"date\"],axis=1).sum(axis=1)\n",
    "pivot_df = grouped_df.pivot(index=\"msno\",columns=\"date\",values=\"sum_vals\")\n",
    "pivot_df = pivot_df.merge(right=y,how=\"left\",on=\"msno\")\n",
    "#pivot_df_churn = pivot_df[(y.sort_values(\"msno\")[\"is_churn\"] == 1).values]\n",
    "#pivot_df_nochurn = pivot_df[(y.sort_values(\"msno\")[\"is_churn\"] == 0).values]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax = sns.heatmap(pivot_df_churn,cbar=False,yticklabels=False,xticklabels=False)\n",
    "\n",
    "with sns.axes_style(\"dark\"):\n",
    "    f, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax = sns.heatmap(pivot_df_nochurn,cbar=False,yticklabels=False,xticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape User Logs and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22411, 215, 7)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape df to numpy 3d matrix\n",
    "padded_df_data = padded_df.iloc[:,2:]\n",
    "num_cols = len(padded_df_data.columns)\n",
    "padded_array = padded_df_data.values.reshape(num_users,num_dates,num_cols)\n",
    "np.save(\"{}_user_logs_padded\".format(split),padded_array)\n",
    "padded_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move numpy file to GCS\n",
    "blob = Blob(\"50_pct_undersample/{}_user_logs_padded.npy\".format(split), bucket)\n",
    "with open(\"{}_user_logs_padded.npy\".format(split), \"rb\") as my_file:\n",
    "    blob.upload_from_file(my_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate by Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload because the norms were taken from daily data\n",
    "user_logs = pd.read_csv(INPUT_PATH + \"{}_user_logs.csv\".format(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove user_logs that don't exist in members df\n",
    "msno_diff = list(set(user_logs.msno) - set(members_df.msno)) #difference in 2 sets\n",
    "user_logs = user_logs[~user_logs[\"msno\"].isin(msno_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip with min=0 to remove invalid counts\n",
    "for col in [\"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"]:\n",
    "    user_logs[col] =  np.clip(user_logs[col].values.reshape(-1,1), a_min=0,a_max=user_logs[col].values.reshape(-1,1).max())\n",
    "\n",
    "user_logs.loc[:,[\"date\"]]=pd.to_datetime(user_logs.date).dt.date\n",
    "user_logs[\"week\"] = pd.to_datetime(user_logs['date']).dt.to_period('W')\n",
    "user_logs_group = user_logs.drop([\"date\"],axis=1)\n",
    "user_logs_week = user_logs_group.groupby([\"msno\",\"week\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize, clip, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and clip the values at 2 * std because we want to be very sensitive at the lower end of the data.\n",
    "# In other words, it's safe to assume that those playing tracks above 2*std won't churn, we want to investigate the distribution of data below that point\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "for col in [\"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"]:\n",
    "    count_clip = np.clip(user_logs_week[col].values.reshape(-1,1), a_min=0,a_max=user_logs_week[col].values.reshape(-1,1).max())\n",
    "    col_log = np.log(count_clip +.0001)\n",
    "    col_std = std_scaler.fit_transform(col_log)\n",
    "    col_clip = np.clip(col_std, a_min=col_std.min(),a_max=2)\n",
    "    col_norm = max_scaler.fit_transform(col_clip)\n",
    "    user_logs_week[col + \"_norm\"] = col_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs_week = user_logs_week.drop([\"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(set(user_logs_week.msno))\n",
    "num_users = len(users)\n",
    "start_date = date(2016,7,1) # Technically we can go back further, but it introduces a lot of missing data for various users\n",
    "end_date = date(2017,1,31) # Max date we care about before evaluating churn\n",
    "#print(f\"Num dates: {num_dates}; num_users: {num_users}; padded records to create: {num_dates*num_users}\")\n",
    "# this creates a list containing all dates from start to end\n",
    "dates = [start_date + timedelta(days=x) for x in range((end_date-start_date).days + 1)]\n",
    "weeks = pd.to_datetime(pd.Series(dates)).dt.to_period('W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_list = list(set(weeks.astype('str')))\n",
    "weeks_list.sort()\n",
    "weeks_list = weeks_list[0:len(weeks_list)-1] # remove last week which includes Feb\n",
    "num_weeks=len(weeks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the user_log data with the dates created in the previous step\n",
    "padded_df = pd.DataFrame(product(users, weeks_list), columns=[\"msno\",\"week\"])\n",
    "# Convert week to string before merge\n",
    "user_logs_week[\"week\"] = user_logs_week.week.astype('str')\n",
    "\n",
    "padded_df = padded_df.merge(user_logs_week, how='left', on=[\"msno\",\"week\"]).fillna(0)\n",
    "padded_df = padded_df.sort_values(by=[\"msno\",\"week\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "padded_df_labeled = padded_df.merge(right=y,how=\"left\",on=\"msno\")\n",
    "\n",
    "padded_df_churn = padded_df_labeled[padded_df_labeled[\"is_churn\"] == 1]\n",
    "padded_df_no_churn = padded_df_labeled[padded_df_labeled[\"is_churn\"] == 0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metric = \"num_unq\"\n",
    "\n",
    "mean_churn_by_date = padded_df_churn.loc[:,[\"week\",metric]].groupby(['week']).mean().reset_index()\n",
    "mean_no_churn_by_date = padded_df_no_churn.loc[:,[\"week\",metric]].groupby(['week']).mean().reset_index()\n",
    "\n",
    "mean_churn_by_date[metric + \"_centered\"] = mean_churn_by_date[metric] - mean_churn_by_date[metric].mean()\n",
    "mean_no_churn_by_date[metric + \"_centered\"] = mean_no_churn_by_date[metric] - mean_no_churn_by_date[metric].mean()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.lineplot(data=mean_churn_by_date.iloc[:-1,:],x=\"week\", y=metric, label=\"Churn\")\n",
    "ax = sns.lineplot(data=mean_no_churn_by_date.iloc[:-1,:],x=\"week\", y=metric, label=\"No Churn\")\n",
    "ax.set(xlabel='Week', ylabel='Avg Unique Plays / Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.lineplot(data=mean_churn_by_date.iloc[:-1,:],x=\"week\", y=metric + \"_centered\", label=\"Churn\")\n",
    "ax = sns.lineplot(data=mean_no_churn_by_date.iloc[:-1,:],x=\"week\", y=metric + \"_centered\", label=\"No Churn\")\n",
    "ax.set(xlabel='Week', ylabel='Avg Unique Plays / Week (Centered)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grouped_df = padded_df.drop(['num_25_norm', 'num_50_norm', 'num_75_norm',\n",
    "       'num_985_norm', 'num_100_norm', 'num_unq_norm', 'total_secs_norm'],axis=1)\n",
    "grouped_df[\"sum\"] = padded_df.num_unq_norm #padded_df.drop([\"msno\",\"date\"],axis=1).sum(axis=1)\n",
    "pivot_df = grouped_df.pivot(index=\"msno\",columns=\"week\",values=\"sum\")\n",
    "pivot_df_churn = pivot_df[(y.sort_values(\"msno\")[\"is_churn\"] == 1).values]\n",
    "pivot_df_nochurn = pivot_df[(y.sort_values(\"msno\")[\"is_churn\"] == 0).values]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax = sns.heatmap(pivot_df_churn,cbar=True,yticklabels=False,xticklabels=False)\n",
    "\n",
    "with sns.axes_style(\"dark\"):\n",
    "    f, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax = sns.heatmap(pivot_df_nochurn,cbar=True,yticklabels=False,xticklabels=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "churn_avg = pivot_df_churn.copy()\n",
    "churn_avg[\"Avg CHURN User\"]=\"foo\"\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(20, 2))\n",
    "    ax = sns.heatmap(churn_avg.groupby(\"Avg CHURN User\").sum()/churn_avg.shape[0],cbar=True,yticklabels=False,xticklabels=False)\n",
    "\n",
    "\n",
    "no_churn_avg = pivot_df_nochurn.copy()\n",
    "no_churn_avg[\"Avg NO CHURN User\"]=\"foo\"\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(20, 2))\n",
    "    ax = sns.heatmap(no_churn_avg.groupby(\"Avg NO CHURN User\").sum()/no_churn_avg.shape[0],cbar=True,yticklabels=False,xticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape df to numpy 3d matrix\n",
    "padded_df_data = padded_df.iloc[:,2:]\n",
    "num_cols = len(padded_df_data.columns)\n",
    "padded_array = padded_df_data.values.reshape(num_users,num_weeks,num_cols)\n",
    "np.save(\"{}_user_logs_weekly_padded\".format(split),padded_array)\n",
    "padded_array.shape\n",
    "# Move numpy file to GCS\n",
    "blob = Blob(\"50_pct_undersample/{}_user_logs_weekly_padded.npy\".format(split), bucket)\n",
    "with open(\"{}_user_logs_weekly_padded.npy\".format(split), \"rb\") as my_file:\n",
    "    blob.upload_from_file(my_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
