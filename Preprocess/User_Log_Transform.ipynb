{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from datetime import datetime,date, timedelta\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import Blob\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "client = storage.Client(project=\"ds5500\")\n",
    "\n",
    "#INPUT_PATH = \"D:\\\\Northeastern\\\\100 pct undersample split\\\\\"\n",
    "INPUT_PATH = \"gs://kkbox-data/data_100_pct_undersample/\"\n",
    "bucket = client.get_bucket(\"kkbox-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=\"train\"\n",
    "user_logs = pd.read_csv(INPUT_PATH + \"X_{}_user_logs.csv\".format(split),usecols=[1,2,3,4,5,6,7,8,9])\n",
    "user_logs.loc[:,[\"date\"]]=pd.to_datetime(user_logs.date).dt.date\n",
    "members_df = pd.read_csv(INPUT_PATH + \"X_{}_transformed.csv\".format(split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "for col in [\"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"]:\n",
    "    max_num = 3 * user_logs[col].values.std()\n",
    "    col_clip = np.clip(user_logs[col].values, a_min=0,a_max=max_num)\n",
    "    col_norm = scaler.fit_transform(col_clip.reshape(-1,1))\n",
    "    user_logs[col + \"_norm\"] = col_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs = user_logs.drop([\"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msno', 'date', 'num_25_norm', 'num_50_norm', 'num_75_norm',\n",
       "       'num_985_norm', 'num_100_norm', 'num_unq_norm', 'total_secs_norm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_logs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = list(set(user_logs.msno))\n",
    "num_users = len(users)\n",
    "start_date = min(user_logs.date)\n",
    "end_date = date(2017,1,31) # Max date we care about before evaluating churn\n",
    "#print(f\"Num dates: {num_dates}; num_users: {num_users}; padded records to create: {num_dates*num_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a list containing all dates from start to end\n",
    "dates = [start_date + timedelta(days=x) for x in range((end_date-start_date).days + 1)]\n",
    "num_dates=len(dates)\n",
    "\n",
    "padded_df = pd.DataFrame(product(users, dates), columns=[\"msno\",\"date\"])\n",
    "padded_df = padded_df.merge(user_logs, how='left', on=[\"msno\",\"date\"]).fillna(0)\n",
    "padded_df = padded_df.sort_values(by=[\"msno\",\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape User Logs and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24798, 762, 7)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape df to numpy 3d matrix\n",
    "padded_df_data = padded_df.iloc[:,2:9]\n",
    "num_cols = len(padded_df_data.columns)\n",
    "padded_array = padded_df_data.values.reshape(num_users,num_dates,num_cols)\n",
    "np.save(\"{}_user_logs_padded\".format(split),padded_array)\n",
    "padded_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move numpy file to GCS\n",
    "blob = Blob(\"data_100_pct_undersample/{}_user_logs_padded.npy\".format(split), bucket)\n",
    "with open(\"{}_user_logs_padded.npy\".format(split), \"rb\") as my_file:\n",
    "    blob.upload_from_file(my_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
